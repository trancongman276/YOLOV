{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-08 14:12:54.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1margs value: Namespace(output_name='yolov_l.onnx', input='images', output='output', opset=11, batch_size=1, dynamic=False, no_onnxsim=False, exp_file='./exps/yolov/yolov_l.py', experiment_name=None, name='yolov_l', ckpt='./yolov_l.pth', opts=[], decode_in_inference=False)\u001b[0m\n",
      "\u001b[32m2023-12-08 14:12:56.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mloading checkpoint done.\u001b[0m\n",
      "c:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "d:\\Project\\Personal\\YOLOV\\yolox\\models\\yolovp_msa.py:887: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  output = [None for _ in range(len(prediction))]\n",
      "d:\\Project\\Personal\\YOLOV\\yolox\\models\\yolovp_msa.py:888: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  output_index = [None for _ in range(len(prediction))]\n",
      "d:\\Project\\Personal\\YOLOV\\yolox\\models\\yolovp_msa.py:890: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  for i, image_pred in enumerate(prediction):\n",
      "d:\\Project\\Personal\\YOLOV\\yolox\\models\\yolovp_msa.py:891: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not image_pred.size(0):\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding:utf-8 -*-\n",
    "# Copyright (c) Megvii, Inc. and its affiliates.\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from loguru import logger\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from yolox.exp import get_exp\n",
    "from yolox.models.network_blocks import SiLU\n",
    "from yolox.utils import replace_module\n",
    "\n",
    "\n",
    "def make_parser():\n",
    "    parser = argparse.ArgumentParser(\"YOLOX onnx deploy\")\n",
    "    parser.add_argument(\n",
    "        \"--output-name\", type=str, default=\"yolox.onnx\", help=\"output name of models\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input\", default=\"images\", type=str, help=\"input node name of onnx model\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\", default=\"output\", type=str, help=\"output node name of onnx model\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-o\", \"--opset\", default=11, type=int, help=\"onnx opset version\"\n",
    "    )\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=1, help=\"batch size\")\n",
    "    parser.add_argument(\n",
    "        \"--dynamic\",\n",
    "        action=\"store_true\",\n",
    "        help=\"whether the input shape should be dynamic or not\",\n",
    "    )\n",
    "    parser.add_argument(\"--no-onnxsim\", action=\"store_true\", help=\"use onnxsim or not\")\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--exp_file\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"experiment description file\",\n",
    "    )\n",
    "    parser.add_argument(\"-expn\", \"--experiment-name\", type=str, default=None)\n",
    "    parser.add_argument(\"-n\", \"--name\", type=str, default=None, help=\"model name\")\n",
    "    parser.add_argument(\"-c\", \"--ckpt\", default=None, type=str, help=\"ckpt path\")\n",
    "    parser.add_argument(\n",
    "        \"opts\",\n",
    "        help=\"Modify config options using the command-line\",\n",
    "        default=None,\n",
    "        nargs=argparse.REMAINDER,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--decode_in_inference\", action=\"store_true\", help=\"decode in inference or not\"\n",
    "    )\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "args = make_parser().parse_args(\"\")\n",
    "args.exp_file = \"./exps/yolov/yolov_l.py\"\n",
    "args.name = \"yolov_l\"\n",
    "args.ckpt = \"./yolov_l.pth\"\n",
    "args.output_name = \"yolov_l.onnx\"\n",
    "\n",
    "\n",
    "logger.info(\"args value: {}\".format(args))\n",
    "exp = get_exp(args.exp_file, args.name)\n",
    "exp.merge(args.opts)\n",
    "\n",
    "if not args.experiment_name:\n",
    "    args.experiment_name = exp.exp_name\n",
    "\n",
    "model = exp.get_model()\n",
    "ckpt_file = args.ckpt\n",
    "\n",
    "# load the model state dict\n",
    "ckpt = torch.load(ckpt_file, map_location=\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "if \"model\" in ckpt:\n",
    "    ckpt = ckpt[\"model\"]\n",
    "model.load_state_dict(ckpt)\n",
    "model = replace_module(model, nn.SiLU, SiLU)\n",
    "model.head.decode_in_inference = args.decode_in_inference\n",
    "model.to(\"cuda\").eval()\n",
    "logger.info(\"loading checkpoint done.\")\n",
    "\n",
    "# For export body\n",
    "dummy_input = torch.randn(5, 3, exp.test_size[0], exp.test_size[1]).cuda()\n",
    "\n",
    "# For export head\n",
    "# dummy_input = {\n",
    "#     \"pred_result\": torch.randn(2, 30, 37).cuda(),\n",
    "#     \"pred_idx\": torch.randn(2, 30).cuda(),\n",
    "# }\n",
    "\n",
    "# input_names = [\"pred_result\", \"pred_idx\"]\n",
    "# output_names = [\"output\"]\n",
    "output_names=[\"decode_res\", \"before_nms_features\", \"before_nms_regf\", \"outputs_decode\"]\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    args.output_name,\n",
    "    input_names=[\"images\"],\n",
    "    output_names=output_names,\n",
    "    dynamic_axes={\"images\": {0: \"batch\"}, \n",
    "                  \"decode_res\": {0: \"batch\"},\n",
    "                  \"before_nms_features\": {0: \"batch\"},\n",
    "                  \"before_nms_regf\": {0: \"batch\"},\n",
    "                  \"outputs_decode\": {0: \"batch\"},},\n",
    "    verbose=True,\n",
    "    do_constant_folding=True,\n",
    "    export_params=True,\n",
    "    # opset_version=args.opset,\n",
    ")\n",
    "logger.info(\"generated onnx model named {}\".format(args.output_name))\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(args.output_name)\n",
    "\n",
    "# Get the model's graph\n",
    "graph = model.graph\n",
    "\n",
    "# Get the list of outputs\n",
    "outputs = graph.output\n",
    "\n",
    "# Remove the desired output\n",
    "# Replace 'output_name' with the name of the output you want to remove\n",
    "del_idx = []\n",
    "for i, output in enumerate(outputs):\n",
    "    if output.name not in output_names:\n",
    "        del_idx.append(i)\n",
    "        logger.info(\"delete output {}\".format(output.name))\n",
    "        \n",
    "del_idx.reverse()\n",
    "for i in del_idx:\n",
    "    del outputs[i]\n",
    "\n",
    "# Save the modified model\n",
    "onnx.save(model, args.output_name)\n",
    "\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "\n",
    "# use onnx-simplifier to reduce reduent model.\n",
    "onnx_model = onnx.load(args.output_name)\n",
    "model_simp, check = simplify(onnx_model)\n",
    "assert check, \"Simplified ONNX model could not be validated\"\n",
    "onnx.save(model_simp, args.output_name)\n",
    "logger.info(\"generated simplified onnx model named {}\".format(args.output_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class WoClass(nn.Module):\n",
    "    def __init__(self, num_classes, Prenum, topK, nms_thre) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.Prenum = Prenum\n",
    "        self.topK = topK\n",
    "        self.nms_thre = nms_thre\n",
    "        \n",
    "    def forward(self, image_pred):\n",
    "        # Get score and class with highest confidence\n",
    "        class_conf, class_pred = torch.max(\n",
    "            image_pred[:, 5 : 5 + self.num_classes], 1, keepdim=True\n",
    "        )\n",
    "\n",
    "        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
    "        detections = torch.cat(\n",
    "            (\n",
    "                image_pred[:, :5],\n",
    "                class_conf,\n",
    "                class_pred.float(),\n",
    "                image_pred[:, 5 : 5 + self.num_classes],\n",
    "            ),\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        conf_score = image_pred[:, 4]\n",
    "        top_pre = torch.topk(conf_score, k=self.Prenum)\n",
    "        sort_idx = top_pre.indices[: self.Prenum]\n",
    "        detections_temp = detections[sort_idx, :]\n",
    "        nms_out_index = torchvision.ops.batched_nms(\n",
    "            detections_temp[:, :4],\n",
    "            detections_temp[:, 4] * detections_temp[:, 5],\n",
    "            detections_temp[:, 6],\n",
    "            self.nms_thre,\n",
    "        )\n",
    "\n",
    "        topk_idx = sort_idx[nms_out_index[: self.topK]]\n",
    "        return detections[topk_idx, :], topk_idx\n",
    "    \n",
    "woClass = WoClass(num_classes=30, \n",
    "                #   Prenum=750,\n",
    "                  Prenum=256,\n",
    "                  topK=30,\n",
    "                  nms_thre=0.75)\n",
    "dummy_input = torch.randn(5376, 35)\n",
    "woClass.eval()\n",
    "woClass(dummy_input)\n",
    "\n",
    "input_names = [\"image_pred\"]\n",
    "output_names = [\"output\", \"output_index\"]\n",
    "dynamic_axes = {\"image_pred\": {0: \"batch\"}, \"output\": {0: \"batch\"}, \"output_index\": {0: \"batch\"}}\n",
    "\n",
    "dummy_input = torch.randn(5376, 35)\n",
    "\n",
    "# torch.onnx.export(\n",
    "#     woClass,\n",
    "#     dummy_input,\n",
    "#     \"WoClass.onnx\",\n",
    "#     input_names=input_names,\n",
    "#     output_names=output_names,\n",
    "#     dynamic_axes=dynamic_axes,\n",
    "#     verbose=True,\n",
    "#     do_constant_folding=True,\n",
    "#     export_params=True,\n",
    "#     # opset_version=args.opset,\n",
    "# )\n",
    "\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postpro_woclass(\n",
    "        prediction, \n",
    "        # num_classes, \n",
    "        # nms_thre=0.75, \n",
    "        # topK=75, \n",
    "        # features=None,\n",
    "    ):\n",
    "        # Prenum = 750\n",
    "        box_corner = prediction.new(prediction.shape)\n",
    "        box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
    "        box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "        box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "        box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "        prediction[:, :, :4] = box_corner[:, :, :4]\n",
    "        outputs = [None for _ in range(len(prediction))]\n",
    "        output_indexs = [None for _ in range(len(prediction))]\n",
    "\n",
    "        for i, image_pred in enumerate(prediction):\n",
    "            if not image_pred.size(0):\n",
    "                continue\n",
    "            output, output_index = woClass(image_pred)\n",
    "            outputs[i] = output\n",
    "            output_indexs[i] = output_index\n",
    "        \n",
    "        outputs = torch.stack(outputs)\n",
    "        output_indexs = torch.stack(output_indexs)\n",
    "        return outputs, output_indexs\n",
    "    \n",
    "def find_feature_score(\n",
    "    features,\n",
    "    idxs,\n",
    "    reg_features,\n",
    "    predictions=None,\n",
    "):\n",
    "    features_cls = []\n",
    "    features_reg = []\n",
    "    cls_scores = []\n",
    "    fg_scores = []\n",
    "    simN = 30\n",
    "    for i, feature in enumerate(features):\n",
    "        features_cls.append(feature[idxs[i][: simN]])\n",
    "        features_reg.append(reg_features[i, idxs[i][: simN]])\n",
    "        cls_scores.append(predictions[i][: simN, 5])\n",
    "        fg_scores.append(predictions[i][: simN, 4])\n",
    "    features_cls = torch.cat(features_cls)\n",
    "    features_reg = torch.cat(features_reg)\n",
    "    cls_scores = torch.cat(cls_scores)\n",
    "    fg_scores = torch.cat(fg_scores)\n",
    "    return features_cls, features_reg, cls_scores, fg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_res = torch.randn(5, 256, 35)\n",
    "pred_result, pred_idx = postpro_woclass(decode_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 30, 37]), torch.Size([5, 30]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result.shape, pred_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-08 14:48:12.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1margs value: Namespace(output_name='yolov_l.onnx', input='images', output='output', opset=11, batch_size=1, dynamic=False, no_onnxsim=False, exp_file='./exps/yolov/yolov_l.py', experiment_name=None, name='yolov_l', ckpt='./yolov_l.pth', opts=[], decode_in_inference=False)\u001b[0m\n",
      "\u001b[32m2023-12-08 14:48:14.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mloading checkpoint done.\u001b[0m\n",
      "d:\\Project\\Personal\\YOLOV\\yolox\\models\\yolovp_msa.py:289: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  [x.flatten(start_dim=2) for x in before_nms_features], dim=2\n",
      "d:\\Project\\Personal\\YOLOV\\yolox\\models\\yolovp_msa.py:294: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  [x.flatten(start_dim=2) for x in before_nms_regf], dim=2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, int, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Project\\Personal\\YOLOV\\export.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m output_names \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mpred_result\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfc_output\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m dynamic_axes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpred_result\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m0\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m}, \u001b[39m\"\u001b[39m\u001b[39mfc_output\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m0\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m}}\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     model\u001b[39m.\u001b[39;49mhead,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     dummy_input,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39myolo_l_head.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     input_names\u001b[39m=\u001b[39;49minput_names,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     output_names\u001b[39m=\u001b[39;49moutput_names,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     do_constant_folding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     export_params\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     \u001b[39m# opset_version=args.opset,\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mgenerated onnx model named \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39myolo_l_head.onnx\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#W1sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnx\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\onnx\\utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[0;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[0;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \n\u001b[0;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     _export(\n\u001b[0;32m    517\u001b[0m         model,\n\u001b[0;32m    518\u001b[0m         args,\n\u001b[0;32m    519\u001b[0m         f,\n\u001b[0;32m    520\u001b[0m         export_params,\n\u001b[0;32m    521\u001b[0m         verbose,\n\u001b[0;32m    522\u001b[0m         training,\n\u001b[0;32m    523\u001b[0m         input_names,\n\u001b[0;32m    524\u001b[0m         output_names,\n\u001b[0;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[0;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[0;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[0;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[0;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[0;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[0;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[0;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[0;32m    533\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\onnx\\utils.py:1596\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m   1593\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1594\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[1;32m-> 1596\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[0;32m   1597\u001b[0m     model,\n\u001b[0;32m   1598\u001b[0m     args,\n\u001b[0;32m   1599\u001b[0m     verbose,\n\u001b[0;32m   1600\u001b[0m     input_names,\n\u001b[0;32m   1601\u001b[0m     output_names,\n\u001b[0;32m   1602\u001b[0m     operator_export_type,\n\u001b[0;32m   1603\u001b[0m     val_do_constant_folding,\n\u001b[0;32m   1604\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[0;32m   1605\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m   1606\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[0;32m   1607\u001b[0m )\n\u001b[0;32m   1609\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[0;32m   1610\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[0;32m   1611\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[0;32m   1612\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\onnx\\utils.py:1135\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[1;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m   1134\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[1;32m-> 1135\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[0;32m   1136\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[0;32m   1138\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\onnx\\utils.py:1011\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[0;32m   1007\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m     )\n\u001b[0;32m   1009\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1011\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[0;32m   1012\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m   1013\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\onnx\\utils.py:915\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m    913\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[0;32m    914\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 915\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[0;32m    916\u001b[0m     model,\n\u001b[0;32m    917\u001b[0m     args,\n\u001b[0;32m    918\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    919\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    920\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    921\u001b[0m )\n\u001b[0;32m    922\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[0;32m    924\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\jit\\_trace.py:1285\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[1;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m   1284\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m-> 1285\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(\n\u001b[0;32m   1286\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[0;32m   1287\u001b[0m )(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1288\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\jit\\_trace.py:133\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[1;32m--> 133\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[0;32m    134\u001b[0m     wrapper,\n\u001b[0;32m    135\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[0;32m    136\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[0;32m    137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[0;32m    138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\jit\\_trace.py:124\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    123\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[1;32m--> 124\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[0;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    126\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\mambaforge\\envs\\yolov\\lib\\site-packages\\torch\\nn\\modules\\module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32md:\\Project\\Personal\\YOLOV\\yolox\\models\\yolovp_msa.py:297\u001b[0m, in \u001b[0;36mYOLOXHead.forward\u001b[1;34m(self, pred_result, pred_idx, before_nms_features, before_nms_regf, outputs_decode, imgs)\u001b[0m\n\u001b[0;32m    288\u001b[0m cls_feat_flatten \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[0;32m    289\u001b[0m     [x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m before_nms_features], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m    290\u001b[0m )\u001b[39m.\u001b[39mpermute(\n\u001b[0;32m    291\u001b[0m     \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m\n\u001b[0;32m    292\u001b[0m )  \u001b[39m# [b,features,channels]\u001b[39;00m\n\u001b[0;32m    293\u001b[0m reg_feat_flatten \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[0;32m    294\u001b[0m     [x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m before_nms_regf], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m    295\u001b[0m )\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 297\u001b[0m features_cls, features_reg, cls_scores, fg_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_feature_score(\n\u001b[0;32m    298\u001b[0m     cls_feat_flatten, pred_idx, reg_feat_flatten, imgs, pred_result\n\u001b[0;32m    299\u001b[0m )\n\u001b[0;32m    300\u001b[0m features_reg \u001b[39m=\u001b[39m features_reg\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m    301\u001b[0m features_cls \u001b[39m=\u001b[39m features_cls\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\Project\\Personal\\YOLOV\\yolox\\models\\yolovp_msa.py:406\u001b[0m, in \u001b[0;36mYOLOXHead.find_feature_score\u001b[1;34m(self, features, idxs, reg_features, imgs, predictions, roi_features)\u001b[0m\n\u001b[0;32m    404\u001b[0m fg_scores \u001b[39m=\u001b[39m []\n\u001b[0;32m    405\u001b[0m \u001b[39mfor\u001b[39;00m i, feature \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(features):\n\u001b[1;32m--> 406\u001b[0m     features_cls\u001b[39m.\u001b[39mappend(feature[idxs[i][: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimN]])\n\u001b[0;32m    407\u001b[0m     features_reg\u001b[39m.\u001b[39mappend(reg_features[i, idxs[i][: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimN]])\n\u001b[0;32m    408\u001b[0m     cls_scores\u001b[39m.\u001b[39mappend(predictions[i][: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimN, \u001b[39m5\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: tensors used as indices must be long, int, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding:utf-8 -*-\n",
    "# Copyright (c) Megvii, Inc. and its affiliates.\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from loguru import logger\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from yolox.exp import get_exp\n",
    "from yolox.models.network_blocks import SiLU\n",
    "from yolox.utils import replace_module\n",
    "\n",
    "\n",
    "def make_parser():\n",
    "    parser = argparse.ArgumentParser(\"YOLOX onnx deploy\")\n",
    "    parser.add_argument(\n",
    "        \"--output-name\", type=str, default=\"yolox.onnx\", help=\"output name of models\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input\", default=\"images\", type=str, help=\"input node name of onnx model\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\", default=\"output\", type=str, help=\"output node name of onnx model\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-o\", \"--opset\", default=11, type=int, help=\"onnx opset version\"\n",
    "    )\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=1, help=\"batch size\")\n",
    "    parser.add_argument(\n",
    "        \"--dynamic\",\n",
    "        action=\"store_true\",\n",
    "        help=\"whether the input shape should be dynamic or not\",\n",
    "    )\n",
    "    parser.add_argument(\"--no-onnxsim\", action=\"store_true\", help=\"use onnxsim or not\")\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--exp_file\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"experiment description file\",\n",
    "    )\n",
    "    parser.add_argument(\"-expn\", \"--experiment-name\", type=str, default=None)\n",
    "    parser.add_argument(\"-n\", \"--name\", type=str, default=None, help=\"model name\")\n",
    "    parser.add_argument(\"-c\", \"--ckpt\", default=None, type=str, help=\"ckpt path\")\n",
    "    parser.add_argument(\n",
    "        \"opts\",\n",
    "        help=\"Modify config options using the command-line\",\n",
    "        default=None,\n",
    "        nargs=argparse.REMAINDER,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--decode_in_inference\", action=\"store_true\", help=\"decode in inference or not\"\n",
    "    )\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "args = make_parser().parse_args(\"\")\n",
    "args.exp_file = \"./exps/yolov/yolov_l.py\"\n",
    "args.name = \"yolov_l\"\n",
    "args.ckpt = \"./yolov_l.pth\"\n",
    "args.output_name = \"yolov_l.onnx\"\n",
    "\n",
    "\n",
    "logger.info(\"args value: {}\".format(args))\n",
    "exp = get_exp(args.exp_file, args.name)\n",
    "exp.merge(args.opts)\n",
    "\n",
    "if not args.experiment_name:\n",
    "    args.experiment_name = exp.exp_name\n",
    "\n",
    "model = exp.get_model()\n",
    "ckpt_file = args.ckpt\n",
    "\n",
    "# load the model state dict\n",
    "ckpt = torch.load(ckpt_file, map_location=\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "if \"model\" in ckpt:\n",
    "    ckpt = ckpt[\"model\"]\n",
    "model.load_state_dict(ckpt)\n",
    "model = replace_module(model, nn.SiLU, SiLU)\n",
    "model.head.decode_in_inference = args.decode_in_inference\n",
    "model.to(\"cuda\")\n",
    "logger.info(\"loading checkpoint done.\")\n",
    "\n",
    "# For export body\n",
    "# dummy_input = torch.randn(3, 3, exp.test_size[0], exp.test_size[1]).cuda()\n",
    "\n",
    "# For export head\n",
    "dummy_input = {\n",
    "    \"pred_result\": torch.randn(5, 30, 37).cuda(),\n",
    "    \"pred_idx\": torch.randn(5, 30).cuda(),\n",
    "    \"before_nms_features\": torch.randn(5, 256, 64, 64).cuda(),\n",
    "    \"before_nms_regf\": torch.randn(5, 256, 32, 32).cuda(),\n",
    "    \"outputs_decode\": torch.randn(5, 256, 16, 16).cuda(),\n",
    "}\n",
    "\n",
    "input_names = [\"pred_result\", \n",
    "               \"pred_idx\",\n",
    "               \"before_nms_features\",\n",
    "               \"before_nms_regf\",\n",
    "               \"outputs_decode\",]\n",
    "output_names = [\"pred_result\", \"fc_output\"]\n",
    "dynamic_axes = {\"pred_result\": {0: \"batch\"}, \"fc_output\": {0: \"batch\"}}\n",
    "torch.onnx.export(\n",
    "    model.head,\n",
    "    dummy_input,\n",
    "    \"yolo_l_head.onnx\",\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    verbose=True,\n",
    "    do_constant_folding=True,\n",
    "    export_params=True,\n",
    "    # opset_version=args.opset,\n",
    ")\n",
    "logger.info(\"generated onnx model named {}\".format(\"yolo_l_head.onnx\"))\n",
    "\n",
    "\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "\n",
    "# use onnx-simplifier to reduce reduent model.\n",
    "onnx_model = onnx.load(\"yolo_l_head.onnx\")\n",
    "model_simp, check = simplify(onnx_model)\n",
    "assert check, \"Simplified ONNX model could not be validated\"\n",
    "onnx.save(model_simp, \"yolo_l_head.onnx\")\n",
    "logger.info(\"generated simplified onnx model named {}\".format(\"yolo_l_head.onnx\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 227 is out of bounds for dimension 0 with size 160",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Project\\Personal\\YOLOV\\export.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cls_feat_flatten \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     [x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m before_nms_features], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\u001b[39m.\u001b[39mpermute(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )  \u001b[39m# [b,features,channels]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m reg_feat_flatten \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     [x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m before_nms_regf], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m features_cls, features_reg, cls_scores, fg_scores \u001b[39m=\u001b[39m find_feature_score(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m             cls_feat_flatten, pred_idx, reg_feat_flatten, pred_result\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m features_reg \u001b[39m=\u001b[39m features_reg\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m features_cls \u001b[39m=\u001b[39m features_cls\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;32md:\\Project\\Personal\\YOLOV\\export.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, feature \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(features):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     features_cls\u001b[39m.\u001b[39mappend(feature[idxs[i][: simN]])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     features_reg\u001b[39m.\u001b[39mappend(reg_features[i, idxs[i][: simN]])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     cls_scores\u001b[39m.\u001b[39mappend(predictions[i][: simN, \u001b[39m5\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X12sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     fg_scores\u001b[39m.\u001b[39mappend(predictions[i][: simN, \u001b[39m4\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: index 227 is out of bounds for dimension 0 with size 160"
     ]
    }
   ],
   "source": [
    "before_nms_features = torch.randn(5, 256, 64, 64)\n",
    "before_nms_regf = torch.randn(5, 256, 32, 32)\n",
    "cls_feat_flatten = torch.cat(\n",
    "    [x.flatten(start_dim=2) for x in before_nms_features], dim=2\n",
    ").permute(\n",
    "    0, 2, 1\n",
    ")  # [b,features,channels]\n",
    "reg_feat_flatten = torch.cat(\n",
    "    [x.flatten(start_dim=2) for x in before_nms_regf], dim=2\n",
    ").permute(0, 2, 1)\n",
    "features_cls, features_reg, cls_scores, fg_scores = find_feature_score(\n",
    "            cls_feat_flatten, pred_idx, reg_feat_flatten, pred_result\n",
    "        )\n",
    "features_reg = features_reg.unsqueeze(0)\n",
    "features_cls = features_cls.unsqueeze(0)\n",
    "cls_scores = cls_scores.to(cls_feat_flatten.dtype)\n",
    "fg_scores = fg_scores.to(cls_feat_flatten.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 320, 64]),\n",
       " torch.Size([5, 30]),\n",
       " torch.Size([256, 160, 32]),\n",
       " torch.Size([5, 30, 37]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_feat_flatten.shape, pred_idx.shape, reg_feat_flatten.shape, pred_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import cv2\n",
    "ort_session = ort.InferenceSession(\"./yolov_l_body.onnx\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "output_name = ort_session.get_outputs()[0].name\n",
    "\n",
    "img = cv2.imread(\"./assets/dog.jpg\")\n",
    "img = cv2.resize(img, (512, 512))\n",
    "imgs = [img, img]\n",
    "imgs = np.stack(imgs, axis=0)\n",
    "imgs = imgs.transpose(0, 3, 1, 2)\n",
    "imgs = imgs.astype(np.float32)\n",
    "imgs /= 255.0\n",
    "imgs = np.ascontiguousarray(imgs)\n",
    "\n",
    "ort_inputs = {input_name: imgs}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_res, before_nms_features, before_nms_regf, outputs_decode, _ = ort_outs\n",
    "decode_res = torch.from_numpy(decode_res)\n",
    "before_nms_features = torch.from_numpy(before_nms_features)\n",
    "before_nms_regf = torch.from_numpy(before_nms_regf)\n",
    "outputs_decode = torch.from_numpy(outputs_decode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5273 is out of bounds for dimension 0 with size 128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Project\\Personal\\YOLOV\\export.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cls_feat_flatten \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     [x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m before_nms_features], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\u001b[39m.\u001b[39mpermute(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )  \u001b[39m# [b,features,channels]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m reg_feat_flatten \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     [x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m before_nms_regf], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m features_cls, features_reg, cls_scores, fg_scores \u001b[39m=\u001b[39m find_feature_score(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             cls_feat_flatten, pred_idx, reg_feat_flatten, pred_result\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m features_reg \u001b[39m=\u001b[39m features_reg\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m features_cls \u001b[39m=\u001b[39m features_cls\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;32md:\\Project\\Personal\\YOLOV\\export.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m simN \u001b[39m=\u001b[39m \u001b[39m30\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, feature \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(features):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     features_cls\u001b[39m.\u001b[39mappend(feature[idxs[i][: simN]])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     features_reg\u001b[39m.\u001b[39mappend(reg_features[i, idxs[i][: simN]])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Personal/YOLOV/export.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     cls_scores\u001b[39m.\u001b[39mappend(predictions[i][: simN, \u001b[39m5\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5273 is out of bounds for dimension 0 with size 128"
     ]
    }
   ],
   "source": [
    "pred_result, pred_idx = postpro_woclass(decode_res)\n",
    "cls_feat_flatten = torch.cat(\n",
    "    [x.flatten(start_dim=2) for x in before_nms_features], dim=2\n",
    ").permute(\n",
    "    0, 2, 1\n",
    ")  # [b,features,channels]\n",
    "reg_feat_flatten = torch.cat(\n",
    "    [x.flatten(start_dim=2) for x in before_nms_regf], dim=2\n",
    ").permute(0, 2, 1)\n",
    "features_cls, features_reg, cls_scores, fg_scores = find_feature_score(\n",
    "            cls_feat_flatten, pred_idx, reg_feat_flatten, pred_result\n",
    "        )\n",
    "features_reg = features_reg.unsqueeze(0)\n",
    "features_cls = features_cls.unsqueeze(0)\n",
    "cls_scores = cls_scores.to(cls_feat_flatten.dtype)\n",
    "fg_scores = fg_scores.to(cls_feat_flatten.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 128, 64]), torch.Size([256, 64, 32]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_feat_flatten.shape, reg_feat_flatten.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
